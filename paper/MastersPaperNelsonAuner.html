<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Master&#39;s Paper Nelson Auner</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h1>Master&#39;s Paper Nelson Auner</h1>

<ol>
<li>Abstract</li>
<li>Introduction</li>
</ol>

<h2>Multinomial Model</h2>

<p>Multinomial models are a common way of modeling annotated text. 
Typically, textual information is grouped by documents and represented by counts of tokens. A document might be a single written text (e.g. an academic article), or a collection of works by the same author (e.g. all of the lyrics of an album by the rolling stones)
A token is often a single word (called unigram) but may also be continguous sequence of two or more words (e.g. &#39;good swimmer&#39; is a bigram, and &#39;I eat cheese&#39; is a trigram).
The word components of tokens are often reduced to a root form by removing suffixes (e.g. &#39;illuminated&#39;, &#39;illumination&#39; and &#39;illuminating&#39; all become &#39;illuminate&#39;).
These tokens are then aggregated by document: for each document \( i \) of \( i = 1,2,...N \), the count vector \( x_i = [x_{i1}, x_{i2}, ... , x_{ip}] \) contains the number of occurrences of first, second, &hellip; \( p \) th token, where \( p \) is the total number of unique tokens in all documents. 
This forms the complete count matrix \( X \), where \( x_{ij} \) is the number of occurences of word \( j \) in document \( i \).</p>

<p><em>example image here?</em></p>

<p>Since the number of unique words that appear in a large number of documents can be extensive, we often restrict \( p \) to words that occur in at least two documents. 
We may also remove common tokens that add little meaning and are found in all documents (i.e. &#39;the&#39; or &#39;of&#39;).</p>

<p>We then model each document \( x_i \) as the realization of a multinomial distribution. That is, </p>

<p>\[  x_{i} \sim MN(q_i,m_i)  \]</p>

<p>Where \( q_i \) is the vector \( [q_{i1}, \dots q_{ip}] \) of token probabilities for document \( x_i \) and \( m_i \) is  \( \sum_{j = 1}^{p}{x_{ij}} \), or the total number of tokens in document \( i \)</p>

<p>It is trivial to show that the maximum likelihood estimator of \( q_i \) is \( f_i \) = \( x_i / m_i \), but by imposing a structure on \( q_i \), we can model features of the data. 
The two most common techniques for creating structure are <em>topic models</em> and  <em>metadata</em></p>

<h2>Topic Models</h2>

<p>A topic model structure assumes that each document is created from a linear combination of \( K \) topics. Each topic \( l = 1,..,K \) represents a distribution, or vector of probability weights \( \omega_l = [\omega_{l1}, ... , \omega_{lp}] \), over words. 
As a simple examine, we can imagine a fitness store that primarily sells books on biking, running, and swimming. 
We can see that a probability distribution of these topics would have high probability weights on the terms (&ldquo;pedal&rdquo;, &ldquo;helment&rdquo;) for biking, (&ldquo;stride&rdquo;) for running, and (&ldquo;breath&rdquo;, &ldquo;stroke&rdquo;, &ldquo;water&rdquo;) for swimming.
By denoting the proportion of topic \( l \) as \( \theta_l \), we can imagine each document as being generated by a linear combination of topics \( \omega_1 \theta_1 + \omega_2 \theta_2 + \omega_3 \theta_3 \), described as the following data-generating process:</p>

<ol>
<li>Choose \( \theta = \theta_1,...\theta_K \) the proportion of topics. (i.e., a book completely about swimming would have \( \theta=(1,0,0) \) , a book about triathalons might have \( \theta =(1/3,1/3,1/3) \) ).</li>
<li>Choose \( m_i \), the number of words in the document</li>
<li>For each word \( j \in 1,2,....,m_i \), choose topic \( l \) with probability \( \theta_l \). With the corresponding weighting vector \( \omega_l \), choose a word \( x_{ij} \) </li>
</ol>

<h2>Metadata</h2>

<p>Text data is frequently accompanied by information, or metadata, about the text itself. For example, in academic journals, metadata on an article could include the number of times the article has been cited, and the journal in which the article has been published.
When this metada is believed to be relevant to the composition of the document, we use the generic term <em>sentiment</em>. For example, given a database of written movie reviews and final rating out of five \( y \in (1,2,3,4,5) \),
we might want to model the relationship between the words used in the document and the final rating.  A simple model introduced by Taddy (2013) is that the contents of a document \( x \) with given metadata rating \( y \), denoted \( x_y \), is</p>

<p>\( x_y \sim MN(q_y,m_y) \) with \( q_{yj} \sim \frac{exp[\alpha_j + y \phi]}{\sum_{l=1}^{p} exp[\alpha_l + y \phi_l]} \)</p>

<p>To determine the linear relationship between metadata \( y \) and count data \( x \), we use Cook&#39;s Inverse Regression method (2007) to reduce the dimension of \( x \) 
while maintaining its predictive power on y. That is, find \( \phi \) such that \( y_i - \phi' x_i \perp x_i \). This criteria is called <em>sufficient reduction</em>.
We then take \( \phi \) and use it to predict content \( x_i \) from metadata \( y_i \). This technique is always helpful and is, in fact, necessary to avoid over-fitting in the many cases where the number of words \( p \) is greater than the number of documents \( N \).
The inverse regression metadata approach has a computational advantage over topic models in that when creating maximum likelihood scores, the word count data \( x_ij \) can be collapsed by metadata label, that is
\( x_y = \sum_{y_{i} = y}{x_i} \)</p>

<p>Recent articles have proposed and implemented versions combining both metadata and topic modeling approaches.</p>

<h1>3. Mixture models and cluster membership</h1>

<p>We now turn our attention to the purpose of this paper, which is to implement an approach to discover and model &ldquo;hidden&rdquo;, or previously unspecified traits, across documents by grouping content unexplained by metadata.<br/>
As a simple motivating example, we might imagine a corpus of movie reviews written by several bloggers. 
After accounting for text information explained by the rating (i.e. relating a 5-star rating to &#39;good plot&#39;, etc), the remaining heterogeneity in the movie review content could be related traits of the blogger (i.e. gender, or home city)
We may be interested in using predicting traits about bloggers given their movie reviews, and also in determining how movie review content changes across these traits.</p>

<h2>Theory and Model Specification</h2>

<p>Denoting the word count of a document as the vector \( x_i \), we propose the following model:
\[  x_{i} \sim MN(q_{ij},m_{ij}) ; q_{ij} = \frac{exp(\alpha_j + y_i \theta_j + u_i \Gamma_{kj})}{\sum{exp(\alpha_j + y_i \theta_j + u_i \Gamma_{kj})}} \]</p>

<p>where \( y_i \), \( u_i \) are the metadata and cluster membership associated with document \( i \), and \( \phi_j \) and \( \Gamma_kj \) are the distortion coeffecients for the respective metadata and factor membership.
We use the subscript \( k \) to denote that each document \( x_i \) is considered a member of \( k = 1,..,K \) clusters, with their own distortion vectors \( \Gamma_1,..,\Gamma_K \)</p>

<p>Our focus in on predicting cluster membership \( u_i \) and the corresponding probability distortion \( \Gamma_i \). 
To do so, we initiatilize cluster membership using one of the three following methods:</p>

<ol>
<li>Random Initialization</li>
<li>K-means on the word count data</li>
<li>K-means on the residual of the word count data after incorporating metadata \( y \)</li>
</ol>

<p>We use an iterated Expectation Maximization algorithm. For each step, we alternate between</p>

<ol>
<li>Determine parameters \( \alpha_, \phi \Gamma \) by fitting a multinomial regression on \( y_i | x_i , u_i \)</li>
<li>For each document \( i \), determine new cluster \( u_i \) membership as \( argmax_{k = 1,..,K}   \ell(y_i,x_i,u_k | \alpha, \phi, \Gamma) \)</li>
</ol>

<p>By alternating between the two steps, we aim to converge to optimal parameter estimates \( \alpha, \phi, \Gamma \) as well as optimal cluster membership \( u \). </p>

<p>To do: </p>

<ol>
<li>Application</li>
<li>Graphs</li>
<li>Conclusion</li>
<li>Acknowledgements</li>
<li>References</li>
</ol>

</body>

</html>

